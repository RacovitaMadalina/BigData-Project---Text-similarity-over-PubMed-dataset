{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"My App\")\n",
    "sc = SparkContext(conf = conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = sqlContext\n",
    "\n",
    "import os, shutil\n",
    "import warnings\n",
    "import random\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from shutil import copyfile\n",
    "from distutils.dir_util import copy_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_paragraphs_list(list_of_paragraphs):\n",
    "    list_of_paragraphs_to_ret = []\n",
    "    if list_of_paragraphs != None:\n",
    "        for paragraph in list_of_paragraphs:\n",
    "            if paragraph != None:\n",
    "                if type(paragraph) == str:\n",
    "                    list_of_paragraphs_to_ret.append(paragraph)\n",
    "                else:\n",
    "                    if paragraph._VALUE != None:\n",
    "                        list_of_paragraphs_to_ret.append(paragraph._VALUE)\n",
    "    return list_of_paragraphs_to_ret\n",
    "\n",
    "def get_paragraphs_list_from_body(input_dir):\n",
    "    df =   spark.read \\\n",
    "                .format('com.databricks.spark.xml') \\\n",
    "                .options(rowTag='record')\\\n",
    "                .options(rowTag='body')\\\n",
    "                .load(input_dir)\n",
    "    return  df.select(\"p\")\\\n",
    "              .rdd\\\n",
    "              .map(lambda row: parse_paragraphs_list(row['p']))\\\n",
    "              .zipWithIndex()\\\n",
    "              .map(lambda record: (record[1], record[0]))\\\n",
    "\n",
    "def build_body(row_sec, row_p):\n",
    "    concat_parsed_sections = ' '.join([' '.join(section) for section in parse_sections_list(row_sec)])\n",
    "    concat_standalone_paragraphs = ' '.join(parse_paragraphs_list(row_p))\n",
    "#     print(concat_parsed_sections, concat_standalone_paragraphs)\n",
    "    return concat_standalone_paragraphs + \" \" + concat_parsed_sections\n",
    "\n",
    "def parse_sections_list(list_of_sections):\n",
    "    if list_of_sections != None:\n",
    "        list_of_sections = [parse_paragraphs_list(section.p) for section in list_of_sections if section.p != None]\n",
    "    else:\n",
    "        list_of_sections = []\n",
    "    return list_of_sections\n",
    "\n",
    "def get_sections_list(input_dir):\n",
    "    df =   spark.read \\\n",
    "                .format('com.databricks.spark.xml') \\\n",
    "                .options(rowTag='record')\\\n",
    "                .options(rowTag='body')\\\n",
    "                .load(input_dir)\n",
    "    return  df.select(\"sec\")\\\n",
    "              .rdd\\\n",
    "              .map(lambda row: parse_sections_list(row['sec']), )\\\n",
    "              .zipWithIndex()\\\n",
    "              .map(lambda record: (record[1], record[0]))\\\n",
    "\n",
    "def get_bodys_list(input_dir):\n",
    "    df =   spark.read \\\n",
    "                .format('com.databricks.spark.xml') \\\n",
    "                .options(rowTag='record')\\\n",
    "                .options(rowTag='body')\\\n",
    "                .load(input_dir)\n",
    "    return  df.select(\"sec\", \"p\")\\\n",
    "              .rdd\\\n",
    "              .map(lambda row: build_body(row['sec'], row[\"p\"]))\\\n",
    "              .zipWithIndex()\\\n",
    "              .map(lambda record: (record[1], record[0]))\n",
    "\n",
    "def get_abstract_list(input_dir):\n",
    "    df =   spark.read \\\n",
    "                .format('com.databricks.spark.xml') \\\n",
    "                .options(rowTag='record')\\\n",
    "                .options(rowTag='abstract')\\\n",
    "                .load(input_dir)\n",
    "    return  df.select(\"sec\", \"p\")\\\n",
    "              .rdd\\\n",
    "              .map(lambda row: build_body(row['sec'], row[\"p\"]))\\\n",
    "              .zipWithIndex()\\\n",
    "              .map(lambda record: (record[1], record[0]))\\\n",
    "\n",
    "\n",
    "def get_article_categories(row):\n",
    "    categories = []\n",
    "    for subj_group in row['subj-group']:\n",
    "        if type(subj_group['subject']) == str:\n",
    "            categories = [subj_group['subject']]\n",
    "        elif type(subj_group['subject']) == list:\n",
    "            for cat in subj_group['subject']:\n",
    "                categories.append(cat)\n",
    "    return categories\n",
    "    \n",
    "\n",
    "def get_categories_list(input_dir):\n",
    "    df = spark.read \\\n",
    "            .format('com.databricks.spark.xml') \\\n",
    "            .options(rowTag='record')\\\n",
    "            .options(rowTag='metadata')\\\n",
    "            .options(rowTag='article')\\\n",
    "            .options(rowTag='front')\\\n",
    "            .options(rowTag='article-meta')\\\n",
    "            .load(input_dir)\n",
    "    \n",
    "    return  df.select(\"article-categories\")\\\n",
    "              .rdd\\\n",
    "              .map(lambda row: get_article_categories(row['article-categories']))\\\n",
    "              .zipWithIndex()\\\n",
    "              .map(lambda record: (record[1], record[0]))\\\n",
    "\n",
    "\n",
    "def get_titles_list(input_dir):\n",
    "    df = spark.read \\\n",
    "            .format('com.databricks.spark.xml') \\\n",
    "            .options(rowTag='record')\\\n",
    "            .options(rowTag='metadata')\\\n",
    "            .options(rowTag='article')\\\n",
    "            .options(rowTag='front')\\\n",
    "            .options(rowTag='article-meta')\\\n",
    "            .options(rowTag='title-group')\\\n",
    "            .load(input_dir)\n",
    "    \n",
    "    return  df.select(\"article-title\")\\\n",
    "              .rdd\\\n",
    "              .map(lambda row: row['article-title'])\\\n",
    "              .zipWithIndex()\\\n",
    "              .map(lambda record: (record[1], record[0]))\\\n",
    "\n",
    "def get_final_rdd(input_dir=\"../datasets/2016_testing_df/\"):\n",
    "    final_rdd = get_abstract_list(input_dir)\n",
    "    final_rdd = final_rdd.join(get_bodys_list(input_dir))\n",
    "    final_rdd = final_rdd.join(get_categories_list(input_dir))\n",
    "    final_rdd = final_rdd.join(get_titles_list(input_dir))\n",
    "#     final_rdd = final_rdd.join(get_sections_list(input_dir))\n",
    "    return final_rdd.sortByKey()\n",
    "\n",
    "def get_final_to_pandas():\n",
    "    return get_final_rdd().map(lambda record: (record[1][0][0][0], record[1][0][0][1], record[1][0][1][0], record[1][1]))\\\n",
    "                          .toDF([\"abstract\", \"body\", \"categories\", \"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_features(sdf, num_features=100):\n",
    "    sdf.registerTempTable('sdf')\n",
    "    new_sdf = sqlContext \\\n",
    "        .sql(\"SELECT CONCAT(abstract, ' ', body, ' ', title) as text, categories as category from sdf\")\n",
    "\n",
    "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "    wordsData = tokenizer.transform(new_sdf)\n",
    "\n",
    "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=num_features)\n",
    "    featurizedData = hashingTF.transform(wordsData)\n",
    "    # alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(featurizedData)\n",
    "    rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "    return rescaledData.select('category', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = get_final_to_pandas()\n",
    "data = create_tfidf_features(sdf, num_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "def run_k_means(data, k=3):\n",
    "    kmeans = KMeans(k=5)\n",
    "    model = kmeans.fit(data.select('features'))\n",
    "    return model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_full = run_k_means(data, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[category: string, features: vector, prediction: int]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code from https://spark.apache.org/docs/1.5.1/ml-features.html#pca and still fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import PCA\n",
    "# from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "# data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "#   (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "#   (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "# df = sqlContext.createDataFrame(data,[\"features\"])\n",
    "# pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "# model = pca.fit(df)\n",
    "# result = model.transform(df).select(\"pcaFeatures\")\n",
    "# result.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}